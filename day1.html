<!DOCTYPE html>
<!-- saved from url=(0045)https://mbzuai-cl.github.io/2023/programday1/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!--<base href=".">--><base href=".">
    <link rel="shortcut icon" type="image/png" href="https://mbzuai-cl.github.io/2023/assets/favicon.png">
    <link rel="stylesheet" type="text/css" media="all" href="./MLLM2024_day1_files/main.css">
    <meta name="description" content="MBZUAI Machine Learning for Large Models Workshop 2024">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>1st MBZUAI Machine Learning for Large Models Workshop 2024</title>
</head>

<body>

    <div class="banner">
        <img src="./MLLM2024_day1_files/banner.jpeg" alt="MBZUAI Machine Learning for Large Models 2024 Banner">
        <div class="top-left">
            <span class="title2">1st MBZUAI Workshop on</span>
            <br><br> <span class="title1">Machine Learning for Large Models</span> 
            <!-- <br><br>
            <span class="year">Empowering Sustainable Futures</span> -->
        </div>
        <div class="bottom-right">
            June 3-4, 2024 <br> MBZUAI, Abu Dhabi
        </div>
    </div>

    <table class="navigation">
        <tbody><tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href="MLLM2024.html">Home</a>
            </td>
            <td class="navigation">
                <a title="Speakers List" href="speakerlist.html">Speakers List</a> 
            </td>
            <td class="navigation">
                <a title="Conference Program" href="day1.html">Program Day 1</a> 
            </td>
            <!-- <td class="navigation">
                <a title="Conference Program" href="https://mbzuai.ac.ae/">MBZUAI</a> 
            </td> -->
            <td class="navigation">
                <a title="Conference Program Day 2" href="day2.html">Program Day 2</a> 
            </td>
        </tr>
    </tbody></table>

   

    <h2>Day 1 Program (June 3, Mon)</h2>

    <table>
        <tbody><tr class="speaker">
            <td class="date" rowspan="2">
                9:00am
            </td>
            <td class="title-special">
                Registration and Opening!
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Here&rsquo;s a section in the program that isn&rsquo;t a talk. 
                Notice that the title is styled differently than the ones for talks. -->
            </td>
        </tr>
    </tbody></table>

    <table id="Hadi Abdine">
        <tbody><tr>
            <td class="date" rowspan="3">
                9:30am
            </td>
            <td class="title">
                Beyond Classification: Multimodal Protein Function Prediction with Prot2Text
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://hadi-abdine.github.io"><b>Hadi Abdine</b></a> (Ecole Polytechnique)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                In recent years, significant advancements have been made in protein function 
                prediction through machine learning. However, most methods limit themselves 
                to multi-classification, assigning predefined labels to proteins. This talk 
                presents Prot2Text, a groundbreaking approach that predicts protein functions 
                in a free text format, transcending traditional binary or categorical classifications. 
                Prot2Text leverages an encoder-decoder framework, combining Graph Neural Networks (GNNs) 
                and Large Language Models (LLMs) to integrate diverse data types such as protein sequences, 
                structures, and textual annotations. This multimodal approach enables a comprehensive 
                representation of protein functions, allowing for the generation of detailed and precise 
                functional descriptions. We will discuss the evaluation of Prot2Text using a multimodal 
                protein dataset extracted from SwissProt and share empirical results demonstrating its 
                effectiveness. This work underscores the transformative potential of integrating GNNs and 
                LLMs, providing researchers with advanced tools for more accurate and nuanced function prediction 
                for both known and novel proteins.
        </td></tr>
    </tbody></table>


    <table id="Timothy Baldwin">
        <tbody><tr>
            <td class="date" rowspan="3">
                9:50am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://eltimster.github.io/www/"><b>Timothy Baldwin</b></a> (MBZUAI and The University of Melbourne)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    <table id="Danqi Chen">
        <tbody><tr>
            <td class="date" rowspan="3">
                10:10am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.cs.princeton.edu/~danqic/"><b>Danqi Chen</b></a> (Princeton University)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>
    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                10:30pm
            </td>
            <td class="title-special">
                Coffee Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>
    <table id="Quanquan Gu">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:00 am
            </td>
            <td class="title">
                Self-Play Preference Optimization for Language Model Alignment
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="http://web.cs.ucla.edu/~qgu/"><b>Quanquan Gu</b></a> (UCLA)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model 
                fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working 
                with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language 
                model alignment. In this talk, I will introduce a self-play-based method for language model alignment, which treats the problem as a 
                constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed \textit{Self-Play Preference Optimization} 
                (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. Our method can 
                effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially 
                achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our 
                experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging 
                a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that 
                achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) 
                DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external 
                supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.
        </td></tr>
    </tbody></table>

    <table id="Samuel Horvath">
        <tbody><tr>
            <td class="date" rowspan="3">
             11:20am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://samuelhorvath.github.io/"><b>Samuel Horvath</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    

    <table id="Nikhil Kandpal">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:40am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://nkandpa2.github.io/"><b>Nikhil Kandpal</b></a> (University of Toronto)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>
    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                12:00pm
            </td>
            <td class="title-special">
                Lunch
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>
    <table id="Salman Khan">
        <tbody><tr>
            <td class="date" rowspan="3">
                1:30pm
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://salman-h-khan.github.io/"><b>Salman Khan</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    <table id="Hector Liu">
        <tbody><tr>
            <td class="date" rowspan="3">
                1:50pm
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://hunterhector.github.io/"><b>Hector Liu</b></a> (Petuum and MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    <table id="Francesco Locatello">
        <tbody><tr>
            <td class="date" rowspan="3">
                2:10pm
            </td>
            <td class="title">
                Are large models enough for causality? 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.francescolocatello.com"><b>Francesco Locatello</b></a> (ISTA)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Machine learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several 
                scientific phenomena. Much of the current progress is driven by scale and, conveniently, many scientific questions require analyzing 
                massive amounts of data. At the same time, in scientific applications predictions are often incorporated into broader analysis to draw 
                new insights that are causal in nature. In this talk I will explore the role of scale in causal analyses. I will start by showing it is 
                possible to extract causal knowledge from a pre-trained statistical model that was trained without causal considerations. Next, I will
                discuss when, how, and why causal directions can be directly predicted by transformers trained with supervised learning in a controlled 
                synthetic setting. Finally, I will discuss the open challenges solving real-world causal downstream tasks in the sciences. For this, I 
                will present ISTAnt, the first real-world benchmark for estimating causal effects from high-dimensional observations in experimental ecology.
        </td></tr>
    </tbody></table>
    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                2:30pm
            </td>
            <td class="title-special">
                Coffee Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>
    <table id="Nils Hendrik Lukas">
        <tbody><tr>
            <td class="date" rowspan="3">
                3:00pm
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://nilslukas.github.io"><b>Nils Hendrik Lukas</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    <table id="Preslav Nakov">
        <tbody><tr>
            <td class="date" rowspan="3">
                3:20pm
            </td>
            <td class="title">
                Factuality Challenges in the Era of Large Language Models: Can We Keep LLMs Safe and Factual?
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/preslav-nakov/"><b>Preslav Nakov</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We will discuss the risks, the challenges, and the opportunities that Large Language Models (LLMs) bring regarding factuality. We will then present a number of LLM fact-checking tools recently developed at MBZUAI: (i) Factcheck-Bench, a fine-grained evaluation benchmark and framework for fact-checking the output of LLMs, (ii) OpenFactVerification (Loki), an open-source tool for fact-checking the output of LLMs, developed based on Factcheck-Bench and optimized for speed and quality, (iii) OpenFactCheck, a framework for building customized fact-checking systems and for benchmarking entire LLMs, and (iv) LM-Polygraph, a tool to predict an LLM's uncertainty in its output using cheap and fast uncertainty quantification techniques. Finally, we will discuss the safety mechanisms we incorporated in Jais-chat, the world's best open Arabic-centric foundation and instruction-tuned LLM.
        </td></tr>
    </tbody></table>

    <table id="Karthik Nandakumar">
        <tbody><tr>
            <td class="date" rowspan="3">
                3:40pm
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.sprintai.org/nkarthik/"><b>Karthik Nandakumar</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    
    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                4:00pm
            </td>
            <td class="title-special">
                Panel Discussion
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>

</body></html>